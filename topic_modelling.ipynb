{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anjaneyatripathi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anjaneyatripathi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_name):\n",
    "    \n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dictionary(sentences):\n",
    "    \n",
    "    # preprocessing the sentences by stemming them\n",
    "    processed_docs = []\n",
    "    for doc in sentences:\n",
    "        processed_docs.append(preprocess(doc))\n",
    "        \n",
    "    # forming the dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # creating the BOWs \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "    return dictionary, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the LDA model\n",
    "def create_model(topics, bow_corpus, dictionary):\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = topics, id2word = dictionary, passes = 10, workers = 2)\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the result of the topics generated\n",
    "def extract_keywords(s):\n",
    "    \n",
    "    words=[]\n",
    "    status=False\n",
    "    word=''\n",
    "    for ch in s:\n",
    "        if(ch=='\"'):\n",
    "            status = not status;\n",
    "        elif(ch=='+'):\n",
    "            status=False\n",
    "            words.append(word)\n",
    "            word=''\n",
    "        elif(status):\n",
    "            word+=ch\n",
    "    words.append(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = load_text('text.txt')\n",
    "topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, bow_corpus = create_bow_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = create_model(3, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.053*\"trade\" + 0.052*\"chan\" + 0.037*\"trial\" + 0.037*\"insid\" + 0.037*\"convict\" + 0.037*\"charg\" + 0.037*\"wang\" + 0.021*\"drug\" + 0.021*\"compani\" + 0.021*\"accord\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.056*\"wang\" + 0.032*\"complaint\" + 0.032*\"secur\" + 0.032*\"charg\" + 0.032*\"offic\" + 0.032*\"exchang\" + 0.032*\"antifraud\" + 0.032*\"provis\" + 0.032*\"thereund\" + 0.032*\"seek\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.070*\"chan\" + 0.049*\"tip\" + 0.049*\"alleg\" + 0.049*\"akebia\" + 0.049*\"wang\" + 0.028*\"drug\" + 0.028*\"approxim\" + 0.028*\"base\" + 0.028*\"later\" + 0.028*\"success\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_topics(topics, lda_model, sentences):\n",
    "\n",
    "    keyword_dict = {}\n",
    "\n",
    "    for i in range(topics):\n",
    "        keyword_dict[i] = extract_keywords(lda_model.print_topics(-1)[i][1])\n",
    "        \n",
    "    TOPICS = {}\n",
    "\n",
    "    for i in range(topics):\n",
    "        TOPICS[i] = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        processed = preprocess(sent)\n",
    "        occurences = [0] * topics\n",
    "        for word in processed:\n",
    "            for i in range(topics):\n",
    "                if(word in keyword_dict[i]):\n",
    "                    occurences[i]+=1\n",
    "        top_hit = max(occurences)\n",
    "        for i in range(topics):\n",
    "            if(occurences[i]==top_hit):\n",
    "                print(i)\n",
    "                TOPICS[i].append(sent)\n",
    "        print(sent)\n",
    "    \n",
    "    return keyword_dict, TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "According to the SEC's complaint, Wang tipped his close friend, Jason \"Schultz\" Chan, to trade Merrimack securities in advance of the company announcing positive drug trial results.\n",
      "2\n",
      "The SEC further alleges that Chan later returned the favor and tipped Wang with nonpublic information about a successful drug trial conducted by Chan's employer, Akebia Therapeutics, Inc.\n",
      "2\n",
      "The complaint alleges Wang made approximately $108,000 trading Akebia securities based on Chan's tips.\n",
      "0\n",
      "The SEC previously charged Chan with insider trading in connection with this investigation.\n",
      "0\n",
      "Both Wang and Chan were charged with insider trading by the U.S. Attorney's Office for the District of Massachusetts and were criminally convicted after a jury trial.\n",
      "0\n",
      "They are currently in the process of appealing their convictions.\n",
      "1\n",
      "The SEC's complaint charges Wang with violating the antifraud provisions of Section 10(b) of the Securities and Exchange Act of 1934 and Rule 10b-5 thereunder and seeks disgorgement of Wang's illegal profits, a civil penalty, an injunction, and an officer-and-director bar.\n"
     ]
    }
   ],
   "source": [
    "keyword_dict, TOPICS = classify_topics(topics, lda_model, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "['According to the SEC\\'s complaint, Wang tipped his close friend, Jason \"Schultz\" Chan, to trade Merrimack securities in advance of the company announcing positive drug trial results.', 'The SEC previously charged Chan with insider trading in connection with this investigation.', \"Both Wang and Chan were charged with insider trading by the U.S. Attorney's Office for the District of Massachusetts and were criminally convicted after a jury trial.\", 'They are currently in the process of appealing their convictions.']\n",
      "Topic # 1\n",
      "[\"The SEC's complaint charges Wang with violating the antifraud provisions of Section 10(b) of the Securities and Exchange Act of 1934 and Rule 10b-5 thereunder and seeks disgorgement of Wang's illegal profits, a civil penalty, an injunction, and an officer-and-director bar.\"]\n",
      "Topic # 2\n",
      "[\"The SEC further alleges that Chan later returned the favor and tipped Wang with nonpublic information about a successful drug trial conducted by Chan's employer, Akebia Therapeutics, Inc.\", \"The complaint alleges Wang made approximately $108,000 trading Akebia securities based on Chan's tips.\"]\n"
     ]
    }
   ],
   "source": [
    "for i in range(topics):\n",
    "    print('Topic #',i)\n",
    "    print(TOPICS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
